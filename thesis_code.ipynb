{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1737,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1774,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measurements = ['mg/m', 'mg/mL', ' g', ' mm', ' mg', 'mg/m2', 'mL/min', 'mL/kg', 'mg/dl', 'mmol/L', 'mEq/dl', 'mm3', ' mL', ' x ', 'mg/kg']\n",
    "def extractSections(regexes):\n",
    "    sections = []\n",
    "    sections_for_search = []\n",
    "    for regex in regexes:\n",
    "        regex_splited = regex[1:-1].split()\n",
    "        flag = True\n",
    "        for measurement in measurements:\n",
    "            if regex.find(measurement) != -1:\n",
    "                flag = False\n",
    "        if not flag:\n",
    "            continue\n",
    "        for symbol in regex:\n",
    "            if symbol.isalpha():\n",
    "                flag = False\n",
    "        if flag:\n",
    "            continue\n",
    "        if re.search(r'[^0-9\\.]', regex_splited[0]):\n",
    "            continue\n",
    "        if re.search(r'[0-9]+ of |[0-9]+ Orteronel', regex):\n",
    "            continue\n",
    "        if re.search(r'\\d+h', regex) or re.search(r'\\d+ *min', regex):\n",
    "            continue\n",
    "        if regex_splited[-1].isdigit():\n",
    "            if int(regex_splited[-1]) > 1980 and int(regex_splited[-1]) < 2015:\n",
    "                continue\n",
    "        if len(regex_splited) < 2:\n",
    "            continue\n",
    "        if regex_splited[0][0] == '0':\n",
    "            continue\n",
    "        if regex_splited[0].isdigit():\n",
    "            if int(regex_splited[0]) > 50:\n",
    "                continue\n",
    "            if regex_splited[1].find('//') != -1 and len(regex_splited[1]) < 8:\n",
    "                continue\n",
    "        sections.append(regex[1:-1])\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1775,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_all_phases_containing_tar_wrd(target_word, tar_passage, left_margin = 10, right_margin = 10):\n",
    "    tokens = nltk.word_tokenize(tar_passage)\n",
    "     \n",
    "    text = nltk.Text(tokens)\n",
    " \n",
    "    c = nltk.ConcordanceIndex(text.tokens, key = lambda s: s.lower())\n",
    " \n",
    "    concordance_txt = ([text.tokens[map(lambda x: x-5 if (x-left_margin)>0 else 0,[offset])[0]:offset+right_margin]\n",
    "                        for offset in c.offsets(target_word)])\n",
    "                         \n",
    "    return [\"...\" + ''.join([x+' ' for x in con_sub]) + \"...\" for con_sub in concordance_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1776,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkClinicalExperience(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write('Searching for the item \"2a Scientific background and explanation of rationale...\"\\n')\n",
    "    section_number = -1\n",
    "    found_section = False\n",
    "    rationale_sections = []\n",
    "    pr_sections = []\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if not found_section:\n",
    "            if int(lower_section[0]) <= 5:\n",
    "                if re.search(r'introduction|background', lower_section): \n",
    "                    fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "                    section_number = lower_section.split()[0].split('.')[0]\n",
    "                    initial_pos = data.find(section)\n",
    "                    found_section = True\n",
    "        elif lower_section.split()[0].split('.')[0] != section_number:\n",
    "            position = data.find(section)\n",
    "            intro = data[initial_pos:position]\n",
    "            break\n",
    "        else:\n",
    "            if re.search(r'rationale', lower_section):\n",
    "                rationale_sections.append(section)\n",
    "            if re.search(r'clinical (activity|experience|studies with|findings)', lower_section):\n",
    "                pr_sections.append(section)\n",
    "            elif re.search(r'phase (I{1,2}|1|2)', lower_section):\n",
    "                pr_sections.append(section)\n",
    "\n",
    "    if not found_section:\n",
    "        fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "        return\n",
    "    intro = intro.replace(\"\\n\", \" \")\n",
    "    if len(rationale_sections) > 0:\n",
    "        joined_rationales = '\\n'.join(rationale_sections)\n",
    "        fl.write(\"For more information about the rationales, go to sections:\\n{0}\\n\".format(joined_rationales))\n",
    "    else:\n",
    "        fl.write(\"I have not found anything about the rationales from titles, lets take a look for the introduction\\n\")\n",
    "        #info = get_all_phases_containing_tar_wrd('rationale', intro.lower())\n",
    "        info = re.findall(r'rationale', intro)\n",
    "        if len(info) == 0:\n",
    "            fl.write(\"Rationales: NOT FOUND\\n\")\n",
    "        else:\n",
    "            fl.write(\"Rationales: FOUND\\n\")\n",
    "            fl.write(\"Occurrences in the text:\\n\")\n",
    "            start_pos = [m.start() for m in re.finditer('rationale', intro)]\n",
    "            for k in start_pos:\n",
    "                    quote = intro[max(k - 40, 0):min(k + 40, len(intro))]\n",
    "                    quote = quote[quote.index(' '):quote.rindex(' ')]\n",
    "                    fl.write('{0} \\t rationale \\t \"...{1}...\"\\n'.format(k, quote))\n",
    "    if len(pr_sections) > 0:\n",
    "        joined_pr = '\\n'.join(pr_sections)\n",
    "        fl.write(\"For more information about the previous studies, go to sections:\\n{0}\\n\".format(joined_pr))\n",
    "    else:\n",
    "        fl.write(\"I have not found anything about the previous studies from titles, lets take a look for the introduction\\n\")\n",
    "        st_oc = set(re.findall(r'[Cc]linical (?:[Aa]ctivity|[Ee]xperience|[Ss]tudies with|[Ff]indings)|[Pp]revious human experience|[Pp]hase (?:I{1,2}|1|2)[., \\n\\t]+', intro))\n",
    "        if len(st_oc) == 0:\n",
    "            fl.write(\"Previous studies: NOT FOUND\\n\")\n",
    "        else:\n",
    "            fl.write(\"Previous studies: FOUND\\n\")\n",
    "            fl.write(\"Occurrences in the text:\\n\")\n",
    "            for phrase in st_oc:\n",
    "                start_pos = [m.start() for m in re.finditer(phrase, intro)]\n",
    "                for k in start_pos:\n",
    "                    quote = intro[max(k - 40, 0):min(k + 40, len(intro))]\n",
    "                    quote = quote[quote.index(' '):quote.rindex(' ')]\n",
    "                    fl.write('{0}\\t{1}\\t\"...{2}...\"\\n'.format(initial_pos + k, phrase, quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1777,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkStudyObjectives(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write('Searching for the item \"2b Specific objectives or hypotheses\"...\\n')\n",
    "    section_number = -1\n",
    "    found_objectives = False\n",
    "    prim = []\n",
    "    sec = []\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if not found_objectives:\n",
    "            if re.search(r'objective|hypothesis|purpose|goal', lower_section): \n",
    "                fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "                if lower_section.split()[0].find('.') != -1:\n",
    "                    section_number = lower_section.split()[0].split('.')[0]\n",
    "                else:\n",
    "                    section_number = lower_section[0]\n",
    "                initial_pos = data.find(section)\n",
    "                found_objectives = True\n",
    "        elif lower_section.split()[0].split('.')[0] != section_number:\n",
    "            position = data.find(section)\n",
    "            intro = data[initial_pos:position]\n",
    "            break\n",
    "        else:\n",
    "            if re.search(r'primary', lower_section):\n",
    "                prim.append(section)\n",
    "            if re.search(r'secondary', lower_section):\n",
    "                sec.append(section)\n",
    "    if not found_objectives:\n",
    "        fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "        return\n",
    "    intro = intro.replace(\"\\n\", \" \")\n",
    "    if len(prim) > 0:\n",
    "        joined_pr = '\\n'.join(prim)\n",
    "        fl.write(\"For more information about the primary objectives, go to sections:\\n{0}\\n\".format(joined_pr))\n",
    "    else:\n",
    "        fl.write(\"Primary objectives: NOT FOUND\\n\")\n",
    "    if len(sec) > 0:\n",
    "        joined_pr = '\\n'.join(sec)\n",
    "        fl.write(\"For more information about the secondary objectives, go to sections:\\n{0}\\n\".format(joined_pr))\n",
    "    else:\n",
    "        fl.write(\"Secondary objectives: NOT FOUND\\n\")\n",
    "    #fl.write(\"I wiil try to find an objective\\n\")   \n",
    "    st_oc = set(re.findall(r'compare |demonstrate |to study |investigate |detect |determine |explore |evaluate |access |quantify |demonstrate |will preserve |to test ', intro))\n",
    "    if len(st_oc) == 0:\n",
    "            fl.write(\"Objectives: NOT FOUND\\n\")\n",
    "    else:\n",
    "        fl.write(\"Objectives: FOUND\\n\")\n",
    "        fl.write(\"Occurrences in the text:\\n\")\n",
    "        for phrase in st_oc:\n",
    "            start_pos = [m.start() for m in re.finditer(phrase, intro)]\n",
    "            for k in start_pos:\n",
    "                quote = ' ' + intro[max(0, k - 5):min(k + 100, len(intro))]\n",
    "                quote = quote[quote.index(' '):quote.rindex(' ')]\n",
    "                fl.write('{0}\\t{1}\\t\"...{2}...\"\\n'.format(initial_pos + k, phrase, quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1778,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkTrialDesign(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write('Searching for the item \"3a Description of trial design (such as parallel, factorial) including allocation ratio\"...\\n')\n",
    "    found_design = False\n",
    "    section_number = -1\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if not found_design:\n",
    "            if re.search(r'study design|study plan', lower_section):\n",
    "                section_number = lower_section.split()[0].split('.')[0]\n",
    "                initial_pos = data.find(section)\n",
    "                fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "                found_design = True\n",
    "        elif section_number != lower_section.split()[0].split('.')[0]:\n",
    "            position = data.find(section)\n",
    "            design = data[initial_pos:position].replace(\"\\n\", \" \")\n",
    "            info = set(re.findall(r'[Pp]hase (?:III|3)|randomi[sz]ed|open[- ]label|multi[- ]*center|double[- ]*blind|non[- ]*blind|international|(?:\\d|two|three)[a-zA-z- ]+(?:arm|regimen)|prospective|placebo[- ]*controlled|active[- ]*controlled', design))\n",
    "            fl.write(\"I found the following features of the study:\\n\")\n",
    "            fl.write('\\n'.join(info))\n",
    "            st_oc = set(re.findall(r'placebo', design))\n",
    "            if len(st_oc) > 0:\n",
    "                fl.write('\\nPlacebo: found -> randomized-controlled study\\n')\n",
    "                fl.write(\"Occurrences in the text:\\n\")\n",
    "                for phrase in st_oc:\n",
    "                    start_pos = [m.start() for m in re.finditer(phrase, design)]\n",
    "                    for k in start_pos:\n",
    "                        quote = design[max(k - 30, 0):min(k + 30, len(design))]\n",
    "                        quote = quote[quote.index(' '):quote.rindex(' ')]\n",
    "                        fl.write('{0}\\t{1}\\t \"...{2}...\"\\n'.format(initial_pos + k, phrase, quote))\n",
    "            else:\n",
    "                fl.write('\\nPlacebo: NOT FOUND -> active-controlled study?\\n')\n",
    "            info = re.findall('\\d(?::\\d)+|equal allocation', design)\n",
    "            if len(info) > 0:\n",
    "                fl.write('Allocation ratio: {0}\\n'.format(', '.join(info)))\n",
    "            else:\n",
    "                fl.write('Allocation ratio: NOT FOUND\\n')\n",
    "            return\n",
    "    fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1779,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkEligibilityCriteria(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write('Checking for the item \"4a Eligibility criteria for participants...\"\\n')\n",
    "    found_criteria = False\n",
    "    section_number = -1\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if not found_criteria:\n",
    "            if re.search(r'selection of patients|study population|(?:inclusion|exclusion|eligibility) criteria|subject eligibility|patient (?:enrollment|definition)', lower_section):\n",
    "                section_number = lower_section.split()[0].split('.')[0]\n",
    "                initial_pos = data.find(section)\n",
    "                fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "                found_criteria = True\n",
    "        elif section_number != lower_section.split()[0].split('.')[0]:\n",
    "            position = data.find(section)\n",
    "            design = data[initial_pos:position].lower()\n",
    "            info = set(re.findall(r'(?:inclusion|exclusion) criteria', design))\n",
    "            if len(info) > 0:\n",
    "                fl.write(\"I have found: {0}\\n\".format(', '.join(info)))\n",
    "            else:\n",
    "                fl.write(\"I have found neither inclusion nor exclusion criteria\\n\")\n",
    "            return\n",
    "    fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1780,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permitted_words = set(['treatment', 'treatments', 'study', 'drug', 'and', 'administration', 'of', 'dosage', 'dosing', 'procedures', 'investigational', 'product', 'medication', 'details', 'interventions'])\n",
    "def checkInterventions(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write('Checking for the item \"5 Interventions for each group with sufficient details to allow replication, including how and when they were actually administered\"\\n')\n",
    "    found_interventions = False\n",
    "    section_number = -1\n",
    "    rel_sections = []\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if not found_interventions:\n",
    "            words_list = nltk.word_tokenize(lower_section)\n",
    "            found = True\n",
    "            for word in words_list:\n",
    "                if not re.search(r'\\b[0-9.]+', word) and word not in permitted_words:\n",
    "                    found = False\n",
    "                    break\n",
    "            if found:\n",
    "                section_number = lower_section.split()[0].split('.')[0]\n",
    "                initial_pos = data.find(section)\n",
    "                fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "                found_interventions = True\n",
    "        elif section_number != lower_section.split()[0].split('.')[0]:\n",
    "            position = data.find(section)\n",
    "            design = data[initial_pos:position].lower()\n",
    "            #info = set(re.findall(r'(?:inclusion|exclusion) criteria', design))\n",
    "            #if len(info) > 0:\n",
    "            #    fl.write(\"I have found: {0}\\n\".format(', '.join(info)))\n",
    "            #else:\n",
    "            #    fl.write(\"I have found neither inclusion nor exclusion criteria\\n\")\n",
    "            if len(rel_sections) > 0:\n",
    "                fl.write('Maybe, these sections will be interested for you:\\n{0}\\n'.format('\\n'.join(rel_sections)))\n",
    "            return\n",
    "        else:\n",
    "            if re.search(r'packaging|labeling|dos(?:ing|age)|schedule|treatment', lower_section):\n",
    "                rel_sections.append(section)\n",
    "    fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1781,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkSampleSize(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write('Checking for the item “7a how sample size was determined”...\\n')\n",
    "    found = False\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if re.search('sample size', lower_section):\n",
    "            fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "            return\n",
    "    fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1782,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkRandomization(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write(\"Searching any information about a randomzation\\n\")\n",
    "    data = data.replace(\"\\n\", \" \")\n",
    "    found = False\n",
    "    initial_pos = 0\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if re.search('randomi[sz]ation|methods? of assign|treatment assign', lower_section):\n",
    "            initial_pos = data.find(section)\n",
    "            fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "    match = set(re.findall(r'dynamic allocation method|permuted-block|balanced blocks', data))\n",
    "    if len(match) > 0:\n",
    "        fl.write(\"I found information about blocks:\\n\")\n",
    "        for phrase in match:\n",
    "            start_pos = [m.start() for m in re.finditer(phrase, data)]\n",
    "            for k in start_pos:\n",
    "                quote = ' ' + data[max(k-50, 0):(k + 50)]\n",
    "                quote = quote[quote.index(' '):quote.rindex(' ')]\n",
    "                fl.write('{0}\\t{1}\\t\"...{2}...\"\\n'.format(initial_pos + k, phrase, quote))\n",
    "    match = set(re.findall(r'randomization will be (?:performed|stratified)|will be randomized|in order to randomize', data))\n",
    "    if len(match) > 0 :\n",
    "        fl.write(\"Maybe this information will be interesting:\\n\")\n",
    "        for phrase in match:\n",
    "            start_pos = [m.start() for m in re.finditer(phrase, data)]\n",
    "            for k in start_pos:\n",
    "                quote = ' ' + data[max(k-100, 0):(k + 100)]\n",
    "                quote = quote[quote.index(' '):quote.rindex(' ')]\n",
    "                fl.write('{0}\\t{1}\\t\"...{2}...\"\\n'.format(k, phrase, quote)) \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1783,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkBlinding(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write(\"Checking for the item “11a  if done, who was blinded after assignment to interventions (eg, participants, care providers, those assessing outcomes) and how.”\\n\")\n",
    "    found = False\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if re.search('study monitoring|blinding|access to randomization code', lower_section):\n",
    "            fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "            found = True\n",
    "    if not found:\n",
    "        fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1784,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkStatistics(sections, data, fl):\n",
    "    fl.write('##########################################\\n')\n",
    "    fl.write(\"Checking for the item “12a  statistical methods used to compare groups for primary and secondary outcomes”\\n\")\n",
    "    found = False\n",
    "    initial_pos = 0\n",
    "    for section in sections:\n",
    "        lower_section = section.lower()\n",
    "        if re.search('(?:statistical (?:methods|considerations|procedures|analys[ie]s))|data analysis', lower_section):\n",
    "            fl.write(\"This part was defined, probably, at this section: {0}\\n\".format(section))\n",
    "            if initial_pos == 0:\n",
    "                initial_pos = data.find(section)\n",
    "            found = True\n",
    "    if not found:\n",
    "        fl.write(\"Unfortunately, I could not find this part, maybe, due to bad conversion to txt file!\\n\")\n",
    "    data = data[initial_pos:].replace(\"\\n\", \" \")\n",
    "    data_l = data.lower()\n",
    "    match = set(re.findall(r'chi[- ]square|kaplan[- ]meier|brookmeyer and crowley|log[- ]?rank| cox |cochran[- ]mantel[- ]haenszel|wilcoxon|fisher[’a-zA-Z ]{1,20}test|fleming|kolmogorov[- ]smirnov|shapiro[- ]wilk|bonferonni|linear slope model', data_l))\n",
    "    if len(match) > 0 :\n",
    "        fl.write(\"I found the following methods:\\n\")\n",
    "        for phrase in match:\n",
    "            start_pos = [m.start() for m in re.finditer(phrase, data_l)]\n",
    "            for k in start_pos:\n",
    "                quote = ' ' + data[max(k-40, 0):(k + 40)]\n",
    "                quote = quote[quote.index(' '):quote.rindex(' ')]\n",
    "                fl.write('{0} \\t {1} \\t \"...{2}...\"\\n'.format(initial_pos + k, phrase, quote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1785,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def processFile(file_path):\n",
    "    alphabet = re.compile('[a-zA-Z0-9-]+|[,./?\\:;=@%^&*()-]+')\n",
    "    fl = open(file_path)\n",
    "    data = fl.read()\n",
    "    data = re.sub('[^\\w,./?\\:;=@%^&*()\\n]', ' ', data)\n",
    "    data = re.sub(r'\\.\\.+', '!', data)\n",
    "    data = re.sub('[\\n]{2,}', '\\n', data)\n",
    "    data = re.sub('[ ]{2,}', ' ', data)\n",
    "    data = re.sub(r'(\\n\\d+[\\.\\d]*[\\r\\t\\v ]*)\\n', r'\\1', data)\n",
    "    data = re.sub(r'(\\d+[\\.\\d]+)([a-zA-Z]+)', r'\\1 \\2', data)\n",
    "    data = re.sub(r'(\\n\\d+[\\r\\t\\v ]*[A-Z0-9]+\\n)(\\d+.[\\d\\.]*[\\r\\t\\v ]*[a-zA-Z0-9- ():/]+\\n)', r'\\1\\n\\2', data)\n",
    "    data = re.sub(r'(\\n\\d+.[\\d\\.]*[\\r\\t\\v ]*[a-zA-Z0-9- ():/]+\\n)(\\d+.[\\d\\.]*[\\r\\t\\v ]*[a-zA-Z0-9- ():/]+\\n)', r'\\1\\n\\2', data)\n",
    "    reg_expressions = re.findall(r'\\n\\d+.[\\d\\.]*[\\r\\t\\v ]*[a-zA-Z0-9- ():/]+\\n|\\n\\d+[\\r\\t\\v ]*[A-Z0-9]+\\n', data)\n",
    "    tokens = sent_tokenize(data)\n",
    "    sections = extractSections(reg_expressions)\n",
    "\n",
    "    #check_clinical_experience(sections, data)\n",
    "    #check_study_objectives(sections, data)\n",
    "    #check_trial_design(sections, data)\n",
    "    return sections, data, tokens\n",
    "    #return reg_expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1786,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findIntersections(sections, data):\n",
    "    found_content = False\n",
    "    start_pos = data.lower().find('table of contents')\n",
    "    if start_pos == -1:\n",
    "        start_pos = data.lower().find('index')\n",
    "        if start_pos == -1:\n",
    "            print 'FAIL'\n",
    "            return sections\n",
    "    end_pos = data[start_pos:].lower().find('reference')\n",
    "    if end_pos != -1:\n",
    "        all_table_of_contents = data[start_pos: start_pos + end_pos + 15]\n",
    "    else:\n",
    "        print 'FAIL'\n",
    "        return sections\n",
    "    valid_sections = []\n",
    "    for section in sections:\n",
    "        section_text = \" \".join(section.split()[1:])\n",
    "        if all_table_of_contents.find(section_text) != -1 or section.split()[0].find('.')  != -1:\n",
    "            valid_sections.append(section)\n",
    "    return valid_sections, start_pos + end_pos + 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1787,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findImportantSections(valid_sections, data, rf):\n",
    "    checkClinicalExperience(valid_sections, data, rf)\n",
    "    checkStudyObjectives(valid_sections, data, rf)\n",
    "    checkTrialDesign(valid_sections, data, rf)\n",
    "    checkEligibilityCriteria(valid_sections, data, rf)\n",
    "    checkInterventions(valid_sections, data, rf)\n",
    "    checkSampleSize(valid_sections, data, rf)\n",
    "    checkRandomization(valid_sections, data, rf)\n",
    "    checkBlinding(valid_sections, data, rf)\n",
    "    checkStatistics(valid_sections, data, rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1788,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protocol with id 79\n",
      "Protocol with id 129\n",
      "Protocol with id 132\n",
      "Protocol with id 134\n",
      "Protocol with id 135\n",
      "Protocol with id 136\n",
      "Protocol with id 137\n",
      "Protocol with id 138\n",
      "Protocol with id 139\n",
      "Protocol with id 140\n",
      "Protocol with id 141\n",
      "Protocol with id 143\n",
      "Protocol with id 144\n",
      "Protocol with id 145\n",
      "Protocol with id 146\n",
      "Protocol with id 148\n",
      "Protocol with id 150\n",
      "Protocol with id 152\n",
      "Protocol with id 153\n",
      "Protocol with id 154\n",
      "Protocol with id 156\n",
      "Protocol with id 163\n",
      "Protocol with id 165\n",
      "Protocol with id 166\n",
      "Protocol with id 168\n",
      "Protocol with id 169\n",
      "Protocol with id 181\n",
      "Protocol with id 184\n",
      "Protocol with id 185\n"
     ]
    }
   ],
   "source": [
    "numbers_of_protocols = [79, 129, 132, 134, 135, 136, 137, 138, 139, 140, 141, 143, 144, 145, 146, 148,\\\n",
    "                      150, 152, 153, 154, 156, 163, 165, 166, 168, 169, 181, 184, 185]\n",
    "\n",
    "#numbers_of_protocols = [79]\n",
    "for id_p in numbers_of_protocols:\n",
    "    report_file = open(\"reports/protocol_\" + str(id_p) + \".txt\", \"w\") \n",
    "    report_file.write(\"Processing the protocol with id {0}\\n\".format(id_p))\n",
    "    print 'Protocol with id', id_p\n",
    "    sections, data, tokens = processFile(\"ready_texts/protocol_\" + str(id_p) + \".txt\")\n",
    "    valid_sections, end_pos = findIntersections(sections, data)\n",
    "    data = data[end_pos:]\n",
    "    sections_file = open(\"sections/protocol_\" + str(id_p) + \".txt\", \"w\") \n",
    "    for section in valid_sections:\n",
    "        sections_file.write(section + '\\n')\n",
    "    text = open(\"normal_sections/protocol_\" + str(id_p) + \".txt\")\n",
    "    oredered_sections = []\n",
    "    for line in text:\n",
    "        oredered_sections.append(line[:-1])\n",
    "    findImportantSections(oredered_sections, data, report_file)\n",
    "    report_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
